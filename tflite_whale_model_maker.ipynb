{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XX46cTrh6iD"
      },
      "source": [
        "##### Copyright 2021 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sKrlWr6Kh-mF"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hST65kOHXpiL"
      },
      "source": [
        "# Transfer Learning for the Audio Domain with TensorFlow Lite Model Maker\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/lite/models/modify/model_maker/audio_classification\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/audio_classification.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/audio_classification.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/tensorflow/tensorflow/lite/g3doc/models/modify/model_maker/audio_classification.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://tfhub.dev/google/yamnet/1\"><img src=\"https://www.tensorflow.org/images/hub_logo_32px.png\" />See TF Hub model</a>\n",
        "  </td>\n",
        "\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BB5k6xNKJ5Xe"
      },
      "source": [
        "\n",
        "In this colab notebook, you'll learn how to use the [TensorFlow Lite Model Maker](https://www.tensorflow.org/lite/models/modify/model_maker) to train a custom audio classification model.\n",
        "\n",
        "The Model Maker library uses transfer learning to simplify the process of training a TensorFlow Lite model using a custom dataset. Retraining a TensorFlow Lite model with your own custom dataset reduces the amount of training data and time required.\n",
        "\n",
        "It is part of the [Codelab to Customize an Audio model and deploy on Android](https://codelabs.developers.google.com/codelabs/tflite-audio-classification-custom-model-android)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeZZ_cSsZfPx"
      },
      "source": [
        "## Intalling dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "wbMc4vHjaYdQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "792c2a46-8f2e-4253-d6c5-c0f79124ab25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libportaudio2 is already the newest version (19.6.0-1build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 24 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libsndfile1 is already the newest version (1.0.28-7ubuntu0.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 24 not upgraded.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tflite-model-maker in /usr/local/lib/python3.9/dist-packages (0.4.2)\n",
            "Requirement already satisfied: lxml>=4.6.1 in /usr/local/lib/python3.9/dist-packages (from tflite-model-maker) (4.9.2)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.9/dist-packages (from tflite-model-maker) (6.0)\n",
            "Requirement already satisfied: tensorflowjs<3.19.0,>=2.4.0 in /usr/local/lib/python3.9/dist-packages (from tflite-model-maker) (3.18.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from tflite-model-maker) (23.3.3)\n",
            "Requirement already satisfied: Cython>=0.29.13 in /usr/local/lib/python3.9/dist-packages (from tflite-model-maker) (0.29.34)\n",
            "Requirement already satisfied: numba==0.53 in /usr/local/lib/python3.9/dist-packages (from tflite-model-maker) (0.53.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tflite-model-maker) (1.16.0)\n",
            "Requirement already satisfied: tflite-support>=0.4.2 in /usr/local/lib/python3.9/dist-packages (from tflite-model-maker) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.9/dist-packages (from tflite-model-maker) (1.23.0)\n",
            "Requirement already satisfied: matplotlib<3.5.0,>=3.0.3 in /usr/local/lib/python3.9/dist-packages (from tflite-model-maker) (3.4.3)\n",
            "Requirement already satisfied: sentencepiece>=0.1.91 in /usr/local/lib/python3.9/dist-packages (from tflite-model-maker) (0.1.99)\n",
            "Requirement already satisfied: tensorflow>=2.6.0 in /usr/local/lib/python3.9/dist-packages (from tflite-model-maker) (2.8.4)\n",
            "Requirement already satisfied: tensorflow-hub<0.13,>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from tflite-model-maker) (0.12.0)\n",
            "Requirement already satisfied: tf-models-official==2.3.0 in /usr/local/lib/python3.9/dist-packages (from tflite-model-maker) (2.3.0)\n",
            "Requirement already satisfied: pillow>=7.0.0 in /usr/local/lib/python3.9/dist-packages (from tflite-model-maker) (8.4.0)\n",
            "Requirement already satisfied: absl-py>=0.10.0 in /usr/local/lib/python3.9/dist-packages (from tflite-model-maker) (1.4.0)\n",
            "Requirement already satisfied: fire>=0.3.1 in /usr/local/lib/python3.9/dist-packages (from tflite-model-maker) (0.5.0)\n",
            "Requirement already satisfied: neural-structured-learning>=1.3.1 in /usr/local/lib/python3.9/dist-packages (from tflite-model-maker) (1.4.0)\n",
            "Requirement already satisfied: scann==1.2.6 in /usr/local/lib/python3.9/dist-packages (from tflite-model-maker) (1.2.6)\n",
            "Requirement already satisfied: librosa==0.8.1 in /usr/local/lib/python3.9/dist-packages (from tflite-model-maker) (0.8.1)\n",
            "Requirement already satisfied: tensorflow-model-optimization>=0.5 in /usr/local/lib/python3.9/dist-packages (from tflite-model-maker) (0.7.4)\n",
            "Requirement already satisfied: tensorflow-addons>=0.11.2 in /usr/local/lib/python3.9/dist-packages (from tflite-model-maker) (0.20.0)\n",
            "Requirement already satisfied: tensorflow-datasets>=2.1.0 in /usr/local/lib/python3.9/dist-packages (from tflite-model-maker) (4.8.3)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from tflite-model-maker) (1.25.11)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from librosa==0.8.1->tflite-model-maker) (3.0.0)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.9/dist-packages (from librosa==0.8.1->tflite-model-maker) (1.2.0)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from librosa==0.8.1->tflite-model-maker) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.10.2 in /usr/local/lib/python3.9/dist-packages (from librosa==0.8.1->tflite-model-maker) (0.12.1)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from librosa==0.8.1->tflite-model-maker) (1.10.1)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.9/dist-packages (from librosa==0.8.1->tflite-model-maker) (1.6.0)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.9/dist-packages (from librosa==0.8.1->tflite-model-maker) (0.4.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from librosa==0.8.1->tflite-model-maker) (20.9)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.9/dist-packages (from librosa==0.8.1->tflite-model-maker) (1.2.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from numba==0.53->tflite-model-maker) (67.7.2)\n",
            "Requirement already satisfied: llvmlite<0.37,>=0.36.0rc1 in /usr/local/lib/python3.9/dist-packages (from numba==0.53->tflite-model-maker) (0.36.0)\n",
            "Requirement already satisfied: tf-slim>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tf-models-official==2.3.0->tflite-model-maker) (1.1.0)\n",
            "Requirement already satisfied: py-cpuinfo>=3.3.0 in /usr/local/lib/python3.9/dist-packages (from tf-models-official==2.3.0->tflite-model-maker) (9.0.0)\n",
            "Requirement already satisfied: google-api-python-client>=1.6.7 in /usr/local/lib/python3.9/dist-packages (from tf-models-official==2.3.0->tflite-model-maker) (2.84.0)\n",
            "Requirement already satisfied: psutil>=5.4.3 in /usr/local/lib/python3.9/dist-packages (from tf-models-official==2.3.0->tflite-model-maker) (5.9.5)\n",
            "Requirement already satisfied: kaggle>=1.3.9 in /usr/local/lib/python3.9/dist-packages (from tf-models-official==2.3.0->tflite-model-maker) (1.5.13)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.9/dist-packages (from tf-models-official==2.3.0->tflite-model-maker) (0.6)\n",
            "Requirement already satisfied: google-cloud-bigquery>=0.31.0 in /usr/local/lib/python3.9/dist-packages (from tf-models-official==2.3.0->tflite-model-maker) (3.9.0)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.9/dist-packages (from tf-models-official==2.3.0->tflite-model-maker) (4.7.0.72)\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.9/dist-packages (from tf-models-official==2.3.0->tflite-model-maker) (0.5.0)\n",
            "Requirement already satisfied: pandas>=0.22.0 in /usr/local/lib/python3.9/dist-packages (from tf-models-official==2.3.0->tflite-model-maker) (1.5.3)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.9/dist-packages (from fire>=0.3.1->tflite-model-maker) (2.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib<3.5.0,>=3.0.3->tflite-model-maker) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib<3.5.0,>=3.0.3->tflite-model-maker) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib<3.5.0,>=3.0.3->tflite-model-maker) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib<3.5.0,>=3.0.3->tflite-model-maker) (2.8.2)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.9/dist-packages (from neural-structured-learning>=1.3.1->tflite-model-maker) (23.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.6.0->tflite-model-maker) (4.5.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.6.0->tflite-model-maker) (2.8.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.6.0->tflite-model-maker) (3.19.6)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.6.0->tflite-model-maker) (0.32.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.6.0->tflite-model-maker) (2.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.6.0->tflite-model-maker) (1.1.2)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.6.0->tflite-model-maker) (0.4.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.9,>=2.8 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.6.0->tflite-model-maker) (2.8.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.6.0->tflite-model-maker) (1.6.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.6.0->tflite-model-maker) (1.53.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.6.0->tflite-model-maker) (1.14.1)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.6.0->tflite-model-maker) (16.0.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.6.0->tflite-model-maker) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.6.0->tflite-model-maker) (3.3.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.6.0->tflite-model-maker) (3.8.0)\n",
            "Requirement already satisfied: typeguard<3.0.0,>=2.7 in /usr/local/lib/python3.9/dist-packages (from tensorflow-addons>=0.11.2->tflite-model-maker) (2.13.3)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.9/dist-packages (from tensorflow-datasets>=2.1.0->tflite-model-maker) (1.13.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from tensorflow-datasets>=2.1.0->tflite-model-maker) (4.65.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from tensorflow-datasets>=2.1.0->tflite-model-maker) (8.1.3)\n",
            "Requirement already satisfied: etils[enp,epath]>=0.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow-datasets>=2.1.0->tflite-model-maker) (1.2.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.9/dist-packages (from tensorflow-datasets>=2.1.0->tflite-model-maker) (0.10.2)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.9/dist-packages (from tensorflow-datasets>=2.1.0->tflite-model-maker) (2.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow-datasets>=2.1.0->tflite-model-maker) (2.27.1)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.9/dist-packages (from tensorflow-datasets>=2.1.0->tflite-model-maker) (0.1.8)\n",
            "Requirement already satisfied: pybind11>=2.6.0 in /usr/local/lib/python3.9/dist-packages (from tflite-support>=0.4.2->tflite-model-maker) (2.10.4)\n",
            "Requirement already satisfied: sounddevice>=0.4.4 in /usr/local/lib/python3.9/dist-packages (from tflite-support>=0.4.2->tflite-model-maker) (0.4.6)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow>=2.6.0->tflite-model-maker) (0.40.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.9/dist-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets>=2.1.0->tflite-model-maker) (5.12.0)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.9/dist-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets>=2.1.0->tflite-model-maker) (3.15.0)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.9/dist-packages (from google-api-python-client>=1.6.7->tf-models-official==2.3.0->tflite-model-maker) (0.21.0)\n",
            "Requirement already satisfied: google-auth<3.0.0dev,>=1.19.0 in /usr/local/lib/python3.9/dist-packages (from google-api-python-client>=1.6.7->tf-models-official==2.3.0->tflite-model-maker) (2.17.3)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.9/dist-packages (from google-api-python-client>=1.6.7->tf-models-official==2.3.0->tflite-model-maker) (4.1.1)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.1.0 in /usr/local/lib/python3.9/dist-packages (from google-api-python-client>=1.6.7->tf-models-official==2.3.0->tflite-model-maker) (0.1.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.9/dist-packages (from google-api-python-client>=1.6.7->tf-models-official==2.3.0->tflite-model-maker) (2.11.0)\n",
            "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from google-cloud-bigquery>=0.31.0->tf-models-official==2.3.0->tflite-model-maker) (2.4.1)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from google-cloud-bigquery>=0.31.0->tf-models-official==2.3.0->tflite-model-maker) (2.3.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.15.0 in /usr/local/lib/python3.9/dist-packages (from google-cloud-bigquery>=0.31.0->tf-models-official==2.3.0->tflite-model-maker) (1.22.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.9/dist-packages (from kaggle>=1.3.9->tf-models-official==2.3.0->tflite-model-maker) (2022.12.7)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.9/dist-packages (from kaggle>=1.3.9->tf-models-official==2.3.0->tflite-model-maker) (8.0.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=0.22.0->tf-models-official==2.3.0->tflite-model-maker) (2022.7.1)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.9/dist-packages (from pooch>=1.0->librosa==0.8.1->tflite-model-maker) (1.4.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->tensorflow-datasets>=2.1.0->tflite-model-maker) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->tensorflow-datasets>=2.1.0->tflite-model-maker) (3.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa==0.8.1->tflite-model-maker) (3.1.0)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.9/dist-packages (from sounddevice>=0.4.4->tflite-support>=0.4.2->tflite-model-maker) (1.15.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.6.0->tflite-model-maker) (0.4.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.6.0->tflite-model-maker) (2.2.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.6.0->tflite-model-maker) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.6.0->tflite-model-maker) (1.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.6.0->tflite-model-maker) (3.4.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow-metadata->tensorflow-datasets>=2.1.0->tflite-model-maker) (1.59.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.9/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->tflite-support>=0.4.2->tflite-model-maker) (2.21)\n",
            "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /usr/local/lib/python3.9/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client>=1.6.7->tf-models-official==2.3.0->tflite-model-maker) (1.48.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3.0.0dev,>=1.19.0->google-api-python-client>=1.6.7->tf-models-official==2.3.0->tflite-model-maker) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3.0.0dev,>=1.19.0->google-api-python-client>=1.6.7->tf-models-official==2.3.0->tflite-model-maker) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3.0.0dev,>=1.19.0->google-api-python-client>=1.6.7->tf-models-official==2.3.0->tflite-model-maker) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow>=2.6.0->tflite-model-maker) (1.3.1)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.9/dist-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery>=0.31.0->tf-models-official==2.3.0->tflite-model-maker) (1.5.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow>=2.6.0->tflite-model-maker) (6.4.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=0.11.15->tensorboard<2.9,>=2.8->tensorflow>=2.6.0->tflite-model-maker) (2.1.2)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.9/dist-packages (from python-slugify->kaggle>=1.3.9->tf-models-official==2.3.0->tflite-model-maker) (1.3)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=1.19.0->google-api-python-client>=1.6.7->tf-models-official==2.3.0->tflite-model-maker) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow>=2.6.0->tflite-model-maker) (3.2.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy==1.23.0 in /usr/local/lib/python3.9/dist-packages (1.23.0)\n"
          ]
        }
      ],
      "source": [
        "!sudo apt -y install libportaudio2\n",
        "!sudo apt -y install libsndfile1\n",
        "!pip install tflite-model-maker\n",
        "!pip install numpy==1.23.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2ck_Ghdcgt9"
      },
      "source": [
        "## Import TensorFlow, Model Maker and other libraries\n",
        "\n",
        "Among the dependencies that are needed, you'll use TensorFlow and Model Maker. Aside those, the others are for audio manipulation, playing and visualizations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "rwUA9u4oWoCR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32bb9d9e-90eb-4cba-da52-3de378915455"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "TensorFlow Version: 2.8.4\n",
            "Model Maker Version: 0.4.2\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import tflite_model_maker as mm\n",
        "from tflite_model_maker import audio_classifier\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import itertools\n",
        "import glob\n",
        "import random\n",
        "\n",
        "from IPython.display import Audio, Image\n",
        "from scipy.io import wavfile\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "print(f\"TensorFlow Version: {tf.__version__}\")\n",
        "print(f\"Model Maker Version: {mm.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ayd7UqCfQQFU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be3c697c-93eb-49d1-f43b-0c54439f3580"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "functions and data structures created\n"
          ]
        }
      ],
      "source": [
        "# @title [Run this] Util functions and data structures.\n",
        "\n",
        "data_dir = 'drive/MyDrive/COS598D/clips/'\n",
        "test_files = os.path.abspath(os.path.join(data_dir, '*/*.wav'))\n",
        "\n",
        "def get_random_audio_file():\n",
        "  test_list = glob.glob(test_files)\n",
        "  random_audio_path = random.choice(test_list)\n",
        "  return random_audio_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "tEeMZh-VQy97"
      },
      "outputs": [],
      "source": [
        "random_audio = get_random_audio_file()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQj1Mf7YZELS"
      },
      "source": [
        "## Training the Model\n",
        "\n",
        "When using Model Maker for audio, you have to start with a model spec. This is the base model that your new model will extract information to learn about the new classes. It also affects how the dataset will be transformed to respect the models spec parameters like: sample rate, number of channels.\n",
        "\n",
        "[YAMNet](https://tfhub.dev/google/yamnet/1) is an audio event classifier trained on the AudioSet dataset to predict audio events from the AudioSet ontology.\n",
        "\n",
        "It's input is expected to be at 16kHz and with 1 channel.\n",
        "\n",
        "You don't need to do any resampling yourself. Model Maker takes care of that for you.\n",
        "\n",
        "- `frame_length` is to decide how long each traininng sample is. in this caase EXPECTED_WAVEFORM_LENGTH * 3s\n",
        "\n",
        "- `frame_steps` is to decide how far appart are the training samples. In this case, the ith sample will start at EXPECTED_WAVEFORM_LENGTH * 6s after the (i-1)th sample.\n",
        "\n",
        "The reason to set these values is to work around some limitation in real world dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "tUcxtfHXY7XS"
      },
      "outputs": [],
      "source": [
        "spec = audio_classifier.YamNetSpec(\n",
        "    keep_yamnet_and_custom_heads=True,\n",
        "    frame_step=3 * audio_classifier.YamNetSpec.EXPECTED_WAVEFORM_LENGTH,\n",
        "    frame_length=6 * audio_classifier.YamNetSpec.EXPECTED_WAVEFORM_LENGTH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EF185yZ_M7zu"
      },
      "source": [
        "## Loading the data\n",
        "\n",
        "Model Maker has the API to load the data from a folder and have it in the expected format for the model spec.\n",
        "\n",
        "The train and test split are based on the folders. The validation dataset will be created as 20% of the train split.\n",
        "\n",
        "Note: The `cache=True` is important to make training later faster but it will also require more RAM to hold the data. For the birds dataset that is not a problem since it's only 300MB, but if you use your own data you have to pay attention to it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "cX0RqETqZgzo"
      },
      "outputs": [],
      "source": [
        "train_data = audio_classifier.DataLoader.from_folder(\n",
        "    spec, os.path.join(data_dir, ''), cache=True)\n",
        "train_data, validation_data = train_data.split(0.8)\n",
        "test_data = audio_classifier.DataLoader.from_folder(\n",
        "    spec, os.path.join(data_dir, ''), cache=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziMghju-Rts2"
      },
      "source": [
        "## Training the model\n",
        "\n",
        "the audio_classifier has the [`create`](https://www.tensorflow.org/lite/api_docs/python/tflite_model_maker/audio_classifier/create) method that creates a model and already start training it. \n",
        "\n",
        "You can customize many parameterss, for more information you can read more details in the documentation.\n",
        "\n",
        "On this first try you'll use all the default configurations and train for 100 epochs.\n",
        "\n",
        "Note: The first epoch takes longer than all the other ones because it's when the cache is created. After that each epoch takes close to 1 second."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "8r6Awvl4ZkIv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08e24746-b1cc-407d-8dbb-aa1b59f6769e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training the model\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " classification_head (Dense)  (None, 2)                2050      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,050\n",
            "Trainable params: 2,050\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "2/2 [==============================] - 240s 45s/step - loss: 0.2160 - acc: 0.9493 - val_loss: 0.0302 - val_acc: 1.0000\n",
            "Epoch 2/100\n",
            "2/2 [==============================] - 0s 287ms/step - loss: 0.0845 - acc: 0.9928 - val_loss: 0.0135 - val_acc: 1.0000\n",
            "Epoch 3/100\n",
            "2/2 [==============================] - 0s 293ms/step - loss: 0.0690 - acc: 0.9855 - val_loss: 0.0093 - val_acc: 1.0000\n",
            "Epoch 4/100\n",
            "2/2 [==============================] - 0s 302ms/step - loss: 0.0672 - acc: 0.9928 - val_loss: 0.0076 - val_acc: 1.0000\n",
            "Epoch 5/100\n",
            "2/2 [==============================] - 0s 284ms/step - loss: 0.0736 - acc: 0.9928 - val_loss: 0.0066 - val_acc: 1.0000\n",
            "Epoch 6/100\n",
            "2/2 [==============================] - 0s 318ms/step - loss: 0.0749 - acc: 0.9928 - val_loss: 0.0059 - val_acc: 1.0000\n",
            "Epoch 7/100\n",
            "2/2 [==============================] - 0s 296ms/step - loss: 0.0774 - acc: 0.9928 - val_loss: 0.0055 - val_acc: 1.0000\n",
            "Epoch 8/100\n",
            "2/2 [==============================] - 0s 315ms/step - loss: 0.0777 - acc: 0.9928 - val_loss: 0.0051 - val_acc: 1.0000\n",
            "Epoch 9/100\n",
            "2/2 [==============================] - 0s 297ms/step - loss: 0.0883 - acc: 0.9928 - val_loss: 0.0048 - val_acc: 1.0000\n",
            "Epoch 10/100\n",
            "2/2 [==============================] - 0s 286ms/step - loss: 0.0835 - acc: 0.9928 - val_loss: 0.0046 - val_acc: 1.0000\n",
            "Epoch 11/100\n",
            "2/2 [==============================] - 0s 289ms/step - loss: 0.0792 - acc: 0.9928 - val_loss: 0.0043 - val_acc: 1.0000\n",
            "Epoch 12/100\n",
            "2/2 [==============================] - 0s 295ms/step - loss: 0.0810 - acc: 0.9928 - val_loss: 0.0041 - val_acc: 1.0000\n",
            "Epoch 13/100\n",
            "2/2 [==============================] - 0s 301ms/step - loss: 0.0804 - acc: 0.9928 - val_loss: 0.0039 - val_acc: 1.0000\n",
            "Epoch 14/100\n",
            "2/2 [==============================] - 0s 288ms/step - loss: 0.0877 - acc: 0.9928 - val_loss: 0.0038 - val_acc: 1.0000\n",
            "Epoch 15/100\n",
            "2/2 [==============================] - 0s 283ms/step - loss: 0.0865 - acc: 0.9928 - val_loss: 0.0037 - val_acc: 1.0000\n",
            "Epoch 16/100\n",
            "2/2 [==============================] - 0s 288ms/step - loss: 0.0742 - acc: 0.9928 - val_loss: 0.0036 - val_acc: 1.0000\n",
            "Epoch 17/100\n",
            "2/2 [==============================] - 0s 317ms/step - loss: 0.0791 - acc: 0.9928 - val_loss: 0.0035 - val_acc: 1.0000\n",
            "Epoch 18/100\n",
            "2/2 [==============================] - 0s 294ms/step - loss: 0.0820 - acc: 0.9928 - val_loss: 0.0035 - val_acc: 1.0000\n",
            "Epoch 19/100\n",
            "2/2 [==============================] - 0s 291ms/step - loss: 0.0749 - acc: 0.9928 - val_loss: 0.0037 - val_acc: 1.0000\n",
            "Epoch 20/100\n",
            "2/2 [==============================] - 1s 481ms/step - loss: 0.0758 - acc: 0.9928 - val_loss: 0.0040 - val_acc: 1.0000\n",
            "Epoch 21/100\n",
            "2/2 [==============================] - 1s 510ms/step - loss: 0.0596 - acc: 0.9928 - val_loss: 0.0043 - val_acc: 1.0000\n",
            "Epoch 22/100\n",
            "2/2 [==============================] - 1s 480ms/step - loss: 0.0648 - acc: 0.9928 - val_loss: 0.0047 - val_acc: 1.0000\n",
            "Epoch 23/100\n",
            "2/2 [==============================] - 1s 542ms/step - loss: 0.0576 - acc: 0.9928 - val_loss: 0.0050 - val_acc: 1.0000\n",
            "Epoch 24/100\n",
            "2/2 [==============================] - 1s 481ms/step - loss: 0.0643 - acc: 0.9928 - val_loss: 0.0055 - val_acc: 1.0000\n",
            "Epoch 25/100\n",
            "2/2 [==============================] - 1s 477ms/step - loss: 0.0557 - acc: 0.9928 - val_loss: 0.0061 - val_acc: 1.0000\n",
            "Epoch 26/100\n",
            "2/2 [==============================] - 1s 464ms/step - loss: 0.0498 - acc: 0.9928 - val_loss: 0.0067 - val_acc: 1.0000\n",
            "Epoch 27/100\n",
            "2/2 [==============================] - 1s 480ms/step - loss: 0.0535 - acc: 0.9928 - val_loss: 0.0074 - val_acc: 1.0000\n",
            "Epoch 28/100\n",
            "2/2 [==============================] - 0s 293ms/step - loss: 0.0470 - acc: 0.9928 - val_loss: 0.0080 - val_acc: 1.0000\n",
            "Epoch 29/100\n",
            "2/2 [==============================] - 0s 291ms/step - loss: 0.0520 - acc: 0.9928 - val_loss: 0.0082 - val_acc: 1.0000\n",
            "Epoch 30/100\n",
            "2/2 [==============================] - 0s 288ms/step - loss: 0.0384 - acc: 0.9928 - val_loss: 0.0083 - val_acc: 1.0000\n",
            "Epoch 31/100\n",
            "2/2 [==============================] - 0s 300ms/step - loss: 0.0479 - acc: 0.9928 - val_loss: 0.0082 - val_acc: 1.0000\n",
            "Epoch 32/100\n",
            "2/2 [==============================] - 0s 312ms/step - loss: 0.0429 - acc: 0.9928 - val_loss: 0.0081 - val_acc: 1.0000\n",
            "Epoch 33/100\n",
            "2/2 [==============================] - 0s 316ms/step - loss: 0.0454 - acc: 0.9928 - val_loss: 0.0079 - val_acc: 1.0000\n",
            "Epoch 34/100\n",
            "2/2 [==============================] - 0s 304ms/step - loss: 0.0513 - acc: 0.9928 - val_loss: 0.0076 - val_acc: 1.0000\n",
            "Epoch 35/100\n",
            "2/2 [==============================] - 0s 295ms/step - loss: 0.0477 - acc: 0.9928 - val_loss: 0.0072 - val_acc: 1.0000\n",
            "Epoch 36/100\n",
            "2/2 [==============================] - 0s 288ms/step - loss: 0.0417 - acc: 0.9928 - val_loss: 0.0067 - val_acc: 1.0000\n",
            "Epoch 37/100\n",
            "2/2 [==============================] - 0s 305ms/step - loss: 0.0445 - acc: 0.9928 - val_loss: 0.0063 - val_acc: 1.0000\n",
            "Epoch 38/100\n",
            "2/2 [==============================] - 0s 298ms/step - loss: 0.0442 - acc: 0.9928 - val_loss: 0.0060 - val_acc: 1.0000\n",
            "Epoch 39/100\n",
            "2/2 [==============================] - 0s 284ms/step - loss: 0.0482 - acc: 0.9928 - val_loss: 0.0057 - val_acc: 1.0000\n",
            "Epoch 40/100\n",
            "2/2 [==============================] - 0s 309ms/step - loss: 0.0452 - acc: 0.9928 - val_loss: 0.0071 - val_acc: 1.0000\n",
            "Epoch 41/100\n",
            "2/2 [==============================] - 0s 293ms/step - loss: 0.0494 - acc: 0.9928 - val_loss: 0.0121 - val_acc: 1.0000\n",
            "Epoch 42/100\n",
            "2/2 [==============================] - 0s 306ms/step - loss: 0.0471 - acc: 0.9928 - val_loss: 0.0180 - val_acc: 1.0000\n",
            "Epoch 43/100\n",
            "2/2 [==============================] - 0s 290ms/step - loss: 0.0426 - acc: 0.9928 - val_loss: 0.0200 - val_acc: 1.0000\n",
            "Epoch 44/100\n",
            "2/2 [==============================] - 0s 294ms/step - loss: 0.0453 - acc: 0.9928 - val_loss: 0.0167 - val_acc: 1.0000\n",
            "Epoch 45/100\n",
            "2/2 [==============================] - 0s 435ms/step - loss: 0.0416 - acc: 0.9928 - val_loss: 0.0119 - val_acc: 1.0000\n",
            "Epoch 46/100\n",
            "2/2 [==============================] - 1s 501ms/step - loss: 0.0417 - acc: 0.9928 - val_loss: 0.0082 - val_acc: 1.0000\n",
            "Epoch 47/100\n",
            "2/2 [==============================] - 1s 488ms/step - loss: 0.0422 - acc: 0.9928 - val_loss: 0.0059 - val_acc: 1.0000\n",
            "Epoch 48/100\n",
            "2/2 [==============================] - 1s 462ms/step - loss: 0.0398 - acc: 0.9928 - val_loss: 0.0046 - val_acc: 1.0000\n",
            "Epoch 49/100\n",
            "2/2 [==============================] - 1s 491ms/step - loss: 0.0399 - acc: 0.9928 - val_loss: 0.0038 - val_acc: 1.0000\n",
            "Epoch 50/100\n",
            "2/2 [==============================] - 1s 524ms/step - loss: 0.0416 - acc: 0.9928 - val_loss: 0.0033 - val_acc: 1.0000\n",
            "Epoch 51/100\n",
            "2/2 [==============================] - 1s 546ms/step - loss: 0.0439 - acc: 0.9928 - val_loss: 0.0030 - val_acc: 1.0000\n",
            "Epoch 52/100\n",
            "2/2 [==============================] - 1s 465ms/step - loss: 0.0421 - acc: 0.9928 - val_loss: 0.0029 - val_acc: 1.0000\n",
            "Epoch 53/100\n",
            "2/2 [==============================] - 0s 424ms/step - loss: 0.0482 - acc: 0.9928 - val_loss: 0.0028 - val_acc: 1.0000\n",
            "Epoch 54/100\n",
            "2/2 [==============================] - 0s 306ms/step - loss: 0.0463 - acc: 0.9928 - val_loss: 0.0028 - val_acc: 1.0000\n",
            "Epoch 55/100\n",
            "2/2 [==============================] - 0s 297ms/step - loss: 0.0470 - acc: 0.9928 - val_loss: 0.0029 - val_acc: 1.0000\n",
            "Epoch 56/100\n",
            "2/2 [==============================] - 0s 294ms/step - loss: 0.0421 - acc: 0.9928 - val_loss: 0.0030 - val_acc: 1.0000\n",
            "Epoch 57/100\n",
            "2/2 [==============================] - 0s 309ms/step - loss: 0.0406 - acc: 0.9928 - val_loss: 0.0033 - val_acc: 1.0000\n",
            "Epoch 58/100\n",
            "2/2 [==============================] - 0s 287ms/step - loss: 0.0349 - acc: 0.9928 - val_loss: 0.0035 - val_acc: 1.0000\n",
            "Epoch 59/100\n",
            "2/2 [==============================] - 0s 330ms/step - loss: 0.0437 - acc: 0.9928 - val_loss: 0.0039 - val_acc: 1.0000\n",
            "Epoch 60/100\n",
            "2/2 [==============================] - 0s 290ms/step - loss: 0.0412 - acc: 0.9928 - val_loss: 0.0043 - val_acc: 1.0000\n",
            "Epoch 61/100\n",
            "2/2 [==============================] - 0s 309ms/step - loss: 0.0368 - acc: 0.9928 - val_loss: 0.0047 - val_acc: 1.0000\n",
            "Epoch 62/100\n",
            "2/2 [==============================] - 0s 298ms/step - loss: 0.0390 - acc: 0.9928 - val_loss: 0.0051 - val_acc: 1.0000\n",
            "Epoch 63/100\n",
            "2/2 [==============================] - 0s 283ms/step - loss: 0.0299 - acc: 0.9928 - val_loss: 0.0054 - val_acc: 1.0000\n",
            "Epoch 64/100\n",
            "2/2 [==============================] - 0s 299ms/step - loss: 0.0363 - acc: 0.9928 - val_loss: 0.0055 - val_acc: 1.0000\n",
            "Epoch 65/100\n",
            "2/2 [==============================] - 0s 290ms/step - loss: 0.0393 - acc: 0.9928 - val_loss: 0.0055 - val_acc: 1.0000\n",
            "Epoch 66/100\n",
            "2/2 [==============================] - 0s 302ms/step - loss: 0.0382 - acc: 0.9928 - val_loss: 0.0082 - val_acc: 1.0000\n",
            "Epoch 67/100\n",
            "2/2 [==============================] - 0s 302ms/step - loss: 0.0394 - acc: 0.9928 - val_loss: 0.0179 - val_acc: 1.0000\n",
            "Epoch 68/100\n",
            "2/2 [==============================] - 0s 289ms/step - loss: 0.0407 - acc: 0.9928 - val_loss: 0.0247 - val_acc: 1.0000\n",
            "Epoch 69/100\n",
            "2/2 [==============================] - 0s 289ms/step - loss: 0.0445 - acc: 0.9928 - val_loss: 0.0190 - val_acc: 1.0000\n",
            "Epoch 70/100\n",
            "2/2 [==============================] - 0s 308ms/step - loss: 0.0348 - acc: 0.9928 - val_loss: 0.0109 - val_acc: 1.0000\n",
            "Epoch 71/100\n",
            "2/2 [==============================] - 0s 422ms/step - loss: 0.0352 - acc: 0.9928 - val_loss: 0.0063 - val_acc: 1.0000\n",
            "Epoch 72/100\n",
            "2/2 [==============================] - 1s 522ms/step - loss: 0.0356 - acc: 0.9928 - val_loss: 0.0041 - val_acc: 1.0000\n",
            "Epoch 73/100\n",
            "2/2 [==============================] - 1s 506ms/step - loss: 0.0394 - acc: 0.9928 - val_loss: 0.0042 - val_acc: 1.0000\n",
            "Epoch 74/100\n",
            "2/2 [==============================] - 1s 506ms/step - loss: 0.0261 - acc: 0.9928 - val_loss: 0.0064 - val_acc: 1.0000\n",
            "Epoch 75/100\n",
            "2/2 [==============================] - 1s 499ms/step - loss: 0.0348 - acc: 0.9928 - val_loss: 0.0089 - val_acc: 1.0000\n",
            "Epoch 76/100\n",
            "2/2 [==============================] - 1s 465ms/step - loss: 0.0368 - acc: 0.9928 - val_loss: 0.0108 - val_acc: 1.0000\n",
            "Epoch 77/100\n",
            "2/2 [==============================] - 1s 417ms/step - loss: 0.0327 - acc: 0.9928 - val_loss: 0.0108 - val_acc: 1.0000\n",
            "Epoch 78/100\n",
            "2/2 [==============================] - 0s 294ms/step - loss: 0.0331 - acc: 0.9928 - val_loss: 0.0092 - val_acc: 1.0000\n",
            "Epoch 79/100\n",
            "2/2 [==============================] - 0s 306ms/step - loss: 0.0336 - acc: 0.9928 - val_loss: 0.0075 - val_acc: 1.0000\n",
            "Epoch 80/100\n",
            "2/2 [==============================] - 0s 292ms/step - loss: 0.0345 - acc: 0.9928 - val_loss: 0.0061 - val_acc: 1.0000\n",
            "Epoch 81/100\n",
            "2/2 [==============================] - 0s 324ms/step - loss: 0.0373 - acc: 0.9928 - val_loss: 0.0074 - val_acc: 1.0000\n",
            "Epoch 82/100\n",
            "2/2 [==============================] - 0s 289ms/step - loss: 0.0297 - acc: 0.9928 - val_loss: 0.0128 - val_acc: 1.0000\n",
            "Epoch 83/100\n",
            "2/2 [==============================] - 0s 299ms/step - loss: 0.0344 - acc: 0.9928 - val_loss: 0.0171 - val_acc: 1.0000\n",
            "Epoch 84/100\n",
            "2/2 [==============================] - 0s 294ms/step - loss: 0.0404 - acc: 0.9928 - val_loss: 0.0159 - val_acc: 1.0000\n",
            "Epoch 85/100\n",
            "2/2 [==============================] - 0s 342ms/step - loss: 0.0289 - acc: 0.9928 - val_loss: 0.0113 - val_acc: 1.0000\n",
            "Epoch 86/100\n",
            "2/2 [==============================] - 0s 304ms/step - loss: 0.0314 - acc: 0.9928 - val_loss: 0.0076 - val_acc: 1.0000\n",
            "Epoch 87/100\n",
            "2/2 [==============================] - 0s 301ms/step - loss: 0.0344 - acc: 0.9928 - val_loss: 0.0053 - val_acc: 1.0000\n",
            "Epoch 88/100\n",
            "2/2 [==============================] - 0s 307ms/step - loss: 0.0328 - acc: 0.9928 - val_loss: 0.0039 - val_acc: 1.0000\n",
            "Epoch 89/100\n",
            "2/2 [==============================] - 0s 283ms/step - loss: 0.0319 - acc: 0.9928 - val_loss: 0.0031 - val_acc: 1.0000\n",
            "Epoch 90/100\n",
            "2/2 [==============================] - 0s 314ms/step - loss: 0.0312 - acc: 0.9928 - val_loss: 0.0026 - val_acc: 1.0000\n",
            "Epoch 91/100\n",
            "2/2 [==============================] - 0s 311ms/step - loss: 0.0369 - acc: 0.9928 - val_loss: 0.0023 - val_acc: 1.0000\n",
            "Epoch 92/100\n",
            "2/2 [==============================] - 0s 291ms/step - loss: 0.0334 - acc: 0.9928 - val_loss: 0.0022 - val_acc: 1.0000\n",
            "Epoch 93/100\n",
            "2/2 [==============================] - 0s 285ms/step - loss: 0.0322 - acc: 0.9928 - val_loss: 0.0022 - val_acc: 1.0000\n",
            "Epoch 94/100\n",
            "2/2 [==============================] - 0s 306ms/step - loss: 0.0348 - acc: 0.9928 - val_loss: 0.0023 - val_acc: 1.0000\n",
            "Epoch 95/100\n",
            "2/2 [==============================] - 1s 463ms/step - loss: 0.0318 - acc: 0.9928 - val_loss: 0.0024 - val_acc: 1.0000\n",
            "Epoch 96/100\n",
            "2/2 [==============================] - 1s 528ms/step - loss: 0.0334 - acc: 0.9928 - val_loss: 0.0026 - val_acc: 1.0000\n",
            "Epoch 97/100\n",
            "2/2 [==============================] - 1s 498ms/step - loss: 0.0331 - acc: 0.9928 - val_loss: 0.0029 - val_acc: 1.0000\n",
            "Epoch 98/100\n",
            "2/2 [==============================] - 1s 556ms/step - loss: 0.0312 - acc: 0.9928 - val_loss: 0.0032 - val_acc: 1.0000\n",
            "Epoch 99/100\n",
            "2/2 [==============================] - 1s 465ms/step - loss: 0.0333 - acc: 0.9928 - val_loss: 0.0035 - val_acc: 1.0000\n",
            "Epoch 100/100\n",
            "2/2 [==============================] - 1s 495ms/step - loss: 0.0300 - acc: 0.9928 - val_loss: 0.0039 - val_acc: 1.0000\n"
          ]
        }
      ],
      "source": [
        "batch_size = 128\n",
        "epochs = 100\n",
        "\n",
        "print('Training the model')\n",
        "model = audio_classifier.create(\n",
        "    train_data,\n",
        "    spec,\n",
        "    validation_data,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXMEHZkAxJTl"
      },
      "source": [
        "The accuracy looks good but it's important to run the evaluation step on the test data and vefify your model achieved good results on unseed data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEM1aRNKtvHS"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "GDoQACMrZnOx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58ed1c4c-4e7a-4018-cb10-d3810d877f8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating the model\n",
            "7/7 [==============================] - 225s 30s/step - loss: 0.0216 - acc: 0.9948\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.021579409018158913, 0.9948453903198242]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "print('Evaluating the model')\n",
        "model.evaluate(test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QRRAM39aOxS"
      },
      "source": [
        "## Understanding your model\n",
        "\n",
        "When training a classifier, it's useful to see the [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix). The confusion matrix gives you detailed knowledge of how your classifier is performing on test data.\n",
        "\n",
        "Model Maker already creates the confusion matrix for you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zqB3c0368iH3"
      },
      "outputs": [],
      "source": [
        "def show_confusion_matrix(confusion, test_labels):\n",
        "  \"\"\"Compute confusion matrix and normalize.\"\"\"\n",
        "  confusion_normalized = confusion.astype(\"float\") / confusion.sum(axis=1)\n",
        "  axis_labels = test_labels\n",
        "  ax = sns.heatmap(\n",
        "      confusion_normalized, xticklabels=axis_labels, yticklabels=axis_labels,\n",
        "      cmap='Blues', annot=True, fmt='.2f', square=True)\n",
        "  plt.title(\"Confusion matrix\")\n",
        "  plt.ylabel(\"True label\")\n",
        "  plt.xlabel(\"Predicted label\")\n",
        "\n",
        "confusion_matrix = model.confusion_matrix(test_data)\n",
        "show_confusion_matrix(confusion_matrix.numpy(), test_data.index_to_label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gr1s7juBy7H"
      },
      "source": [
        "## Testing the model [Optional]\n",
        "\n",
        "You can try the model on a sample audio from the test dataset just to see the results.\n",
        "\n",
        "First you get the serving model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PmlmTl42Bq_u"
      },
      "outputs": [],
      "source": [
        "serving_model = model.create_serving_model()\n",
        "\n",
        "print(f'Model\\'s input shape and type: {serving_model.inputs}')\n",
        "print(f'Model\\'s output shape and type: {serving_model.outputs}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQsZFO2mrYhx"
      },
      "source": [
        "Coming back to the random audio you loaded earlier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uixOfKSUj_9m"
      },
      "source": [
        "The model created has a fixed input window. \n",
        "\n",
        "For a given audio file, you'll have to split it in windows of data of the expected size. The last window might need to be filled with zeros."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YAvGKQL0lNty"
      },
      "outputs": [],
      "source": [
        "sample_rate, audio_data = wavfile.read(random_audio, 'rb')\n",
        "\n",
        "audio_data = np.array(audio_data) / tf.int16.max\n",
        "input_size = serving_model.input_shape[1]\n",
        "\n",
        "splitted_audio_data = tf.signal.frame(audio_data, input_size, input_size, pad_end=True, pad_value=0)\n",
        "\n",
        "print(f'Test audio path: {random_audio}')\n",
        "print(f'Original size of the audio data: {len(audio_data)}')\n",
        "print(f'Number of windows for inference: {len(splitted_audio_data)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLxKd0eFkMcR"
      },
      "source": [
        "You'll loop over all the splitted audio and apply the model for each one of them.\n",
        "\n",
        "The model you've just trained has 2 outputs: The original YAMNet's output and the one you've just trained. This is important because the real world environment is more complicated than just bird sounds. You can use the YAMNet's output to filter out non relevant audio, for example, on the birds use case, if YAMNet is not classifying Birds or Animals, this might show that the output from your model might have an irrelevant classification.\n",
        "\n",
        "\n",
        "Below both outpus are printed to make it easier to understand their relation. Most of the mistakes that your model make are when YAMNet's prediction is not related to your domain (eg: birds)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-8fJLrxGwYT"
      },
      "outputs": [],
      "source": [
        "print(random_audio)\n",
        "\n",
        "results = []\n",
        "print('Result of the window ith:  your model class -> score,  (spec class -> score)')\n",
        "for i, data in enumerate(splitted_audio_data):\n",
        "  yamnet_output, inference = serving_model(data)\n",
        "  results.append(inference[0].numpy())\n",
        "  result_index = tf.argmax(inference[0])\n",
        "  spec_result_index = tf.argmax(yamnet_output[0])\n",
        "  t = spec._yamnet_labels()[spec_result_index]\n",
        "  result_str = f'Result of the window {i}: ' \\\n",
        "  f'\\t{test_data.index_to_label[result_index]} -> {inference[0][result_index].numpy():.3f}, ' \\\n",
        "  f'\\t({spec._yamnet_labels()[spec_result_index]} -> {yamnet_output[0][spec_result_index]:.3f})'\n",
        "  print(result_str)\n",
        "\n",
        "\n",
        "results_np = np.array(results)\n",
        "mean_results = results_np.mean(axis=0)\n",
        "result_index = mean_results.argmax()\n",
        "print(f'Mean result: {test_data.index_to_label[result_index]} -> {mean_results[result_index]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yASrikBgZ9ZO"
      },
      "source": [
        "## Exporting the model\n",
        "\n",
        "The last step is exporting your model to be used on embedded devices or on the browser.\n",
        "\n",
        "The `export` method export both formats for you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Xw_ehPxAdQlz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a24adfc-d3a9-48d3-91fe-6833e8e1d77a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exporing the TFLite model to drive/MyDrive/COS598D/tflite_models/\n"
          ]
        }
      ],
      "source": [
        "models_path = 'drive/MyDrive/COS598D/tflite_models/'\n",
        "print(f'Exporing the TFLite model to {models_path}')\n",
        "\n",
        "model.export(models_path, tflite_filename='whale_tflite_model_1.tflite')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjZRKmurA3y_"
      },
      "source": [
        "You can also export the SavedModel version for serving or using on a Python environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "veBwppOsA-kn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5de80159-2807-4b57-d58f-079f30be2dc7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ],
      "source": [
        "model.export(models_path, export_format=[mm.ExportFormat.SAVED_MODEL, mm.ExportFormat.LABEL])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we move on to using quantization aware training to further reduce the size of the ML model. We'll save it too."
      ],
      "metadata": {
        "id": "tOY1Yrc5PB49"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_model_optimization as tfmot\n",
        "import pathlib\n",
        "import time"
      ],
      "metadata": {
        "id": "QcyYkiGRPAox"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "converter = tf.lite.TFLiteConverter.from_saved_model(models_path+'/saved_model/')\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_quant_model = converter.convert()\n",
        "\n",
        "quant_model_dir = pathlib.Path(models_path+'/saved_model/')/'quantized_model1.tflite'\n",
        "\n",
        "quant_model_dir.write_bytes(tflite_quant_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pNHBAhgPTSF",
        "outputId": "334e1053-0b1b-47d5-a6b6-40fa20c8b95f"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4041264"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, read the saved model back as a TFLite model"
      ],
      "metadata": {
        "id": "R8J8dj9rU4FQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "interpreter = tf.lite.Interpreter(model_path=models_path+\"/quantized_model/quantized_model1.tflite\")\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "input_shape = input_details[0]['shape']\n",
        "print(input_shape)\n",
        "\n",
        "i=0\n",
        "whales_identified=0\n",
        "whale_samples=0\n",
        "other_samples_identified=0\n",
        "other_samples=0\n",
        "directory_mn='drive/MyDrive/COS598D/clips/mn/'\n",
        "directory_not_mn='drive/MyDrive/COS598D/clips/not_mn/'\n",
        "\n",
        "start_time=time.time()\n",
        "for file in os.listdir(directory_mn):\n",
        "  if i==1:\n",
        "    break\n",
        "  filename = os.fsdecode(file)\n",
        "  # if this isn't a .wav file, skip\n",
        "  if filename[-4:] != '.wav':\n",
        "      continue\n",
        "  # try opening the file\n",
        "  waveform, sample_rate = tf.audio.decode_wav(tf.io.read_file(directory_mn+filename))\n",
        "\n",
        "  waveform = tf.squeeze(tf.expand_dims(waveform,0), [2])  # makes a batch of size 1\n",
        "  print(input_details[0])\n",
        "  interpreter.set_tensor(input_details[0]['index'], waveform)\n",
        "  score = interpreter.invoke()\n",
        "  if len(score[0]) == 0:\n",
        "      continue\n",
        "  score = score[0][0]\n",
        "  if filename[0:2]=='Mn':\n",
        "      whale_samples+=1\n",
        "      if score >= 0.5:\n",
        "          whales_identified+=1\n",
        "  else:\n",
        "      other_samples+=1\n",
        "      if score < 0.5:\n",
        "          other_samples_identified+=1\n",
        "  i+=1\n",
        "    \n",
        "for file in os.listdir(directory_not_mn):\n",
        "  break\n",
        "  filename = os.fsdecode(file)\n",
        "  # if this isn't a .wav file, skip\n",
        "  if filename[-4:] != '.wav':\n",
        "      continue\n",
        "      \n",
        "  # try opening the file\n",
        "  try:\n",
        "      waveform, sample_rate = tf.audio.decode_wav(tf.io.read_file(directory_not_mn+filename))\n",
        "  except:\n",
        "      print(\"error encountered while processing \"+filename+\", continuing with inference\")\n",
        "      continue\n",
        "  waveform = tf.expand_dims(waveform,0)\n",
        "  try:\n",
        "      score = m.predict(waveform)\n",
        "  except:\n",
        "      print('could not run inference on ' + filename)\n",
        "      continue\n",
        "  if len(score[0]) == 0:\n",
        "      continue\n",
        "  score = score[0][0]\n",
        "  if filename[0:2]=='Mn':\n",
        "      whale_samples+=1\n",
        "      if score >= 0.5:\n",
        "          whales_identified+=1\n",
        "  else:\n",
        "      other_samples+=1\n",
        "      if score < 0.5:\n",
        "          other_samples_identified+=1\n",
        "  i+=1\n",
        "        \n",
        "\n",
        "end_time = time.time()\n",
        "print('done processing data, elapsed time: '+str(end_time-start_time))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "KXHYctHLU2n3",
        "outputId": "701d0ee2-9693-4990-bb59-fb7a992fa33a"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[    1 15600]\n",
            "{'name': 'serving_default_audio:0', 'index': 0, 'shape': array([    1, 15600], dtype=int32), 'shape_signature': array([   -1, 15600], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-68db61e18595>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m   \u001b[0mwaveform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwaveform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# makes a batch of size 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_details\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m   \u001b[0minterpreter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_details\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'index'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwaveform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m   \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minterpreter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/lite/python/interpreter.py\u001b[0m in \u001b[0;36mset_tensor\u001b[0;34m(self, tensor_index, value)\u001b[0m\n\u001b[1;32m    696\u001b[0m       \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minterpreter\u001b[0m \u001b[0mcould\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mset\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m     \"\"\"\n\u001b[0;32m--> 698\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpreter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSetTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mresize_tensor_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot set tensor: Dimension mismatch. Got 42640 but expected 15600 for dimension 1 of input 0."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xr0idac6xfi"
      },
      "source": [
        "## Next Steps\n",
        "\n",
        "You did it.\n",
        "\n",
        "Now your new model can be deployed on  mobile devices using [TFLite AudioClassifier Task API](https://www.tensorflow.org/lite/inference_with_metadata/task_library/audio_classifier).\n",
        "\n",
        "You can also try the same process with your own data with different classes and here is the documentation for [Model Maker for Audio Classification](https://www.tensorflow.org/lite/api_docs/python/tflite_model_maker/audio_classifier).\n",
        "\n",
        "Also learn from end-to-end reference apps: [Android](https://github.com/tensorflow/examples/tree/master/lite/examples/sound_classification/android/), [iOS](https://github.com/tensorflow/examples/tree/master/lite/examples/sound_classification/ios)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}